{
  "hash": "99bdac834dec8658458218d3f5c00351",
  "result": {
    "engine": "jupyter",
    "markdown": "# Eigenschaften\n\n\n\n## Terminiertheit\n\nTerminiertheit bedeutet, dass ein Algorithmus nach endlich vielen Schritten anhält, oder er bricht kontrolliert ab. Einfache Beispiele:\n\n* Addition zweier Dezimalzahlen\n* Summe der ersten N natürlichen Zahlen\n\nAllerdings kann die Terminiertheit nicht für alle Algerithmen gezeigt werden. Das [Halteproblem](https://de.wikipedia.org/wiki/Halteproblem) besagt, dass es gibt keinen Verfahren gibt, welches immer zutreffend sagen kann, ob der Algorithmus für die Eingabe terminiert. Hierzu kann das [Collatz-Problem](https://de.wikipedia.org/wiki/Collatz-Problem) als Beispiel herangezogen werden. \n\nDie Zahlenfolge wird wie folgt konstruiert: \n\n* beginne mit irgendeiner natürlichen Zahl $\\mathsf n_0 > 0$\n* ist $\\mathsf n_i$ gerade so ist $\\mathsf n_{i+1} = n_i/2$\n* ist $\\mathsf n_i$ ungerade so ist $\\mathsf n_{i+1} = 3n_i + 1$\n* endet bei $\\mathsf n_i = 1$\n\nCollatz-Vermutung: Jede so konstruierte Zahlenfolge mündet in den Zyklus 4, 2, 1, egal, mit welcher natürlichen Zahl man beginnt. Bisher unbewiesen.\n\n## Determiniertheit\n\nEin deterministischer Algorithmus ist ein Algorithmus, bei dem nur definierte und reproduzierbare Zustände auftreten. Die Ergebnisse des Algorithmus sind somit immer reproduzierbar. Beispiele hierfür:\n\n* Addition ganzer Zahlen\n* Selectionsort\n* Collatz-Sequenz\n\n## Effizienz\n\nDie Effizienz eines Algorithmus ist nicht strikt definiert und kann folgende Aspekte umfassen: \n\n* Laufzeit\n* Speicheraufwand\n* Energieverbrauch\n\nBei bestimmten Anwendungen sind entsprechende Eigenschaften notwendig:\n\n* Speicheraufwand bei *Big Data*, also riesige Datenmengen, z.B. in der Bioinformatik\n* Laufzeit bei Echtzeitanwendung, z.B. Flugzeugsteuerung, Fußgängerdynamik\n\n## Komplexität\n\nBei der Analyse von Algorithmen, gilt es die Komplexiät zu bestimmen, welche ein Maß für den Aufwand darstellt. Dabei wird nach einer Aufwandfunktion $\\mathsf f(n)$ gesucht, welche von der Problemgröße $\\mathsf n$ abhängt. Beispiel für eine Problemgröße:\n\n* Anzahl der Summanden bei einer Summe \n* Anzahl der zu sortierenden Zahlen\n\nMeist wird dabei die Bestimmung auf eine asymptotische Analyse, d.h. eine einfache Vergleichsfunktion $\\mathsf g(n)$ mit $\\mathsf n \\rightarrow \\infty$, reduziert. Dabei beschränkt $\\mathsf g(n)$ das Wachstum von $\\mathsf f(n)$.\n\nDie Funktion $\\mathsf g(n)$ wird oft durch ein $\\mathcal{O}$ gekennzeichnet und gibt so eine möglichst einfache Vergleichsfunktion an. Beispiele:\n\n* $\\mathsf f_1(n) = n^4 + 5n^2 - 10 \\approx \\mathcal{O}(n^4) = g_1(n)$ \n* $\\mathsf f_2(n) = 2^{n+1} \\approx \\mathcal{O}(2^n) = g_2(n)$ \n\n\n\n![Komplexität eines Algorithmus durch Vergleich einer Aufwandfunktion mit einer Vergleichsfunktion](00-bilder/komplexitaet.svg)\n\nUm sich ein besseres Bild zu den Auswirkungen hoher Kompexitäten zu machen, sei folgendes Beispiel gegeben.\n\n* ein Berechnungsschritt (unabhängig von der Problemgröße $\\mathsf n$) sei z.B. 1 s lang\n* das $\\mathsf n$ sei beispielsweise 1000\n\nDamit ergeben sich folgende (asymptotische) Abschätzungen der Laufzeit:\n\n* $\\mathcal{O}(n)$: 10<sup>3</sup> s ≈ 1 h \n* $\\mathcal{O}(n^2)$: 10<sup>6</sup> s ≈ 11 d \n* $\\mathcal{O}(n^3)$: 10<sup>9</sup> s ≈ 31 a \n* $\\mathcal{O}(2^n)$: 2<sup>1000</sup> s ≈ ...\n\n### Komplexität Selectionsort\n\nDie Kompexität dieses Verfahrens kann leicht abgeschätzt werden. Bei jedem Durchlauf wir das Minimum / Maximum gesucht, was anfangs $\\mathsf n$ Operationen benötigt. Beim nächsten Durchlauf sind es nur noch $\\mathsf n − 1$ Operationen und so weiter. In der Summe sind es also \n\n$$ \\mathsf f(n) = \\sum_{i=0}^n i = \\frac{n(n-1)}{2} \\approx \\mathcal{O}(n^2) $$\n\nDamit hat der Selectionsort eine Komplexität von $\\mathcal{O}(n^2)$. Die folgende Abbildung verdeutlicht dies nochmals.\n\n\n\n\n\n\n\n::: {#457498de .cell quarto-private-1='{\"key\":\"jupyter\",\"value\":{\"outputs_hidden\":true,\"source_hidden\":true}}' tags='[\"remove_cell\"]' execution_count=6}\n\n::: {.cell-output .cell-output-stdout}\n```\n10 41.989\n20 180.766\n30 419.547\n40 757.118\n50 1193.255\n60 1732.344\n70 2365.573\n80 3104.131\n90 3941.413\n100 4881.544\n```\n:::\n:::\n\n\n\n\n![Abschätzung der Koplexität des Selectionsort-Algorithmus](00-bilder/sort_selection.svg)\n\n### Komplexität Bubblesort\n\nDie Komplexität des Bubblesort muss unterschieden werden in den günstigsten Fall (best case), den ungünstigsten Fall (worst case) und einem durchschnittlichen Fall (average case):\n\n* best case: $\\mathcal{O} (n)$\n* worst case: $\\mathcal{O} (n^2)$ \n* average case: $\\mathcal{O} (n^2)$\n\nDer best case ergibt sich zum Beispiel, falls die Eingabeliste bereits sortiert ist, da der Algorithmus nur einmal durch die Liste gehen muss, entsprechend n-Mal. Folgende Abbildung verdeutlicht die Anzahl der durchgeführten Operationen im Falle einer vollständig zufälligen Liste und einer, bei welcher 95% der Werte bereits sortiert ist. Dabei wurden für jedes $\\mathsf n$ jeweils 10000 Listen sortiert. Es ist der Mittelwert und die minimalen und maximalen Operationen dargestellt.\n\n::: {#89baeecd .cell tags='[\"remove_cell\"]' execution_count=8}\n\n::: {.cell-output .cell-output-stdout}\n```\n10 42.0579\n20 181.0355\n30 419.407\n40 756.5538\n50 1194.3781\n60 1732.2329\n70 2368.4465\n80 3105.4017\n90 3941.7954\n100 4878.1801\n```\n:::\n:::\n\n\n\n\n![Abschätzung der Koplexität des Bubblesort-Algorithmus ohne Vorsortierung](00-bilder/sort_bubble_p000.svg)\n\n::: {#d6a27956 .cell tags='[\"remove_cell\"]' execution_count=10}\n\n::: {.cell-output .cell-output-stdout}\n```\n10 30.1403\n20 126.7865\n30 303.873\n40 543.9807\n50 907.6736\n60 1316.7905\n70 1860.8113\n80 2447.6622\n90 3206.3051\n100 3969.7775\n```\n:::\n:::\n\n\n\n\n![Abschätzung der Koplexität des Bubblesort-Algorithmus mit einer 95%-igen Vorsortierung](00-bilder/sort_bubble_p095.svg)\n\n",
    "supporting": [
      "eigenschaften_files"
    ],
    "filters": [],
    "includes": {}
  }
}