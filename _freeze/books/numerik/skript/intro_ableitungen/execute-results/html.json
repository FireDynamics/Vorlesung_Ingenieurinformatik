{
  "hash": "019b753e61c34dc7445a5e725eb407f9",
  "result": {
    "engine": "jupyter",
    "markdown": "## Differentiation\n\nDie numerische Bestimmung von Ableitungen wird hier anhand von zwei Ansätzen demonstriert. Zum Einen als Differenzenquotienten und zum Anderen über das Polynomfitting. Angewendet werden diese Verfahren z.B. beim Suchen von Extrema in Experimental- oder Simulationsdaten, beim Lösen von Differentialgleichungen oder bei Optimierungsverfahren. \n\nObwohl die analytische Bildung einer Ableitung oft viel einfacher ist als die Integration, ist dies in den oben genannten Fällen nicht direkt möglich. Gesucht ist hierbei immer die Ableitung $\\mathsf{f'(x)}$ einer Funktion $\\mathsf{f(x)}$ oder einer diskreten Punktmenge $\\mathsf{(x_i, y_i)}$ an einer bestimmten Stelle $\\mathsf{x=x_0}$ oder auf einem Intervall.  \n\nDie Grundidee bei den hier vorgestellten Differenzenquotienten bzw. Differenzenformeln ist die Annäherung der abzuleitenden Funktion mit einer Taylor-Entwicklung an mehreren Stellen. Damit kann nach der gesuchte Ableitung an der entsprechenden Entwicklungsstelle aufgelöst werden.  \n\n## Taylor-Entwicklung\n\n\n\nMittels der [Taylor-Entwicklung](https://de.wikipedia.org/wiki/Taylorreihe) kann jede beliebig oft stetig differenzierbare Funktion $\\mathsf{f(x)}$ um einem Entwicklungspunkt $\\mathsf{x_0}$ beliebig genau angenähert werden. Die funktionale Abhängigkeit bezieht sich nun auf die Variable $\\mathsf{h}$, welche nur in direkter Umgebung um $\\mathsf{x_0}$ betrachtet wird. Die Taylor-Entwicklung lautet:\n\n$$ \\mathsf{f(x_0 + h) = \\sum_{i=0}^{\\infty}\\frac{1}{i!}f^{(i)}(x_0)\\cdot h^i} $$\n$$ \\mathsf{ = f(x_0) + f'(x_0)\\cdot h + \\frac{1}{2} f''(x_0)\\cdot h^2 + \\frac{1}{6}f'''(x_0)\\cdot h^3 + \\cdots} $$\n\nDiese Entwicklung kann auch nur bis zu einer vorgegebenen Ordnung betrachtet werden. So nimmt die Entwicklung bis zur Ordnung $\\mathsf{ \\mathcal{O}(h^3)}$ folgende Form an:\n\n$$\\mathsf{ f(x_0 + h) = f(x_0) + f'(x_0)\\cdot h + \\frac{1}{2} f''(x_0)\\cdot h^2 + \\mathcal{O}(h^3)} $$\n\nHierbei deutet das Landau-Symbol $\\mathsf{\\mathcal{O}}$ die Ordnung an, welche die vernachlässigten Terme, hier ab \n$\\mathsf{h^3}$, als Approximationsfehler zusammenfasst. Die Ordnung gibt an wie schnell bzw. mit welchem funktionalem Zusammenhang der Approximationsfehler gegen Null läuft für $\\mathsf{h \\rightarrow 0}$.\n\nEine graphische Darstellung der ersten Elemente der Reihe verdeutlichen nochmals die Grundidee. Das folgende Beispiel entwickelt die Funktion\n\n$$ \\mathsf{f(x) = \\sin(3x) + 2x} $$\n\nam Punkt $\\mathsf{x_0=0.85}$.\n\n::: {#9bddf28b .cell tags='[\"hide_input\"]' execution_count=2}\n``` {.python .cell-code}\ndef fkt(x, p=0):\n    if p==0:\n        return np.sin(3*x) + 2*x\n    if p==1:\n        return 3*np.cos(3*x) + 2\n    if p==2:\n        return -9*np.sin(3*x)\n    if p==3:\n        return -27*np.cos(3*x)\n    return None\n\n# Daten für die Visualisierung\nx = np.linspace(0, 2, 100)\ny = fkt(x, p=0)\n```\n:::\n\n\n::: {#e5291e86 .cell tags='[\"hide_input\"]' execution_count=3}\n``` {.python .cell-code}\nx0 = 0.85\n\n# Taylor-Elemente\nte = []\nte.append(0*(x-x0) + fkt(x0, p=0))\nte.append((x-x0) * fkt(x0, p=1))\nte.append((x-x0)**2 * fkt(x0, p=2) * 1/2)\nte.append((x-x0)**3 * fkt(x0, p=3) * 1/6)\n```\n:::\n\n\n::: {#97da7e69 .cell tags='[\"hide_input\"]' execution_count=4}\n``` {.python .cell-code}\nplt.plot(x, y, color='Grey', lw=3, label=\"Funktion\")\nplt.plot(x, te[0], label=\"$\\mathsf{\\mathcal{O}(1)}$\")\nplt.plot(x, te[0] + te[1], label=\"$\\mathsf{\\mathcal{O}(h)}$\")\nplt.plot(x, te[0] + te[1] + te[2], label=\"$\\mathsf{\\mathcal{O}(h^2)}$\")\nplt.plot(x, te[0] + te[1] + te[2] + te[3], label=\"$\\mathsf{\\mathcal{O}(h^3)}$\")\n\nplt.vlines(x0, ymin=0, ymax=fkt(x0), color='Grey', ls='--', alpha=0.5)\n\nplt.ylim([0,4])\n\nplt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left')\nplt.grid()\nplt.xlabel('x')\nplt.ylabel('y');\n```\n\n::: {.cell-output .cell-output-display}\n![](intro_ableitungen_files/figure-html/cell-5-output-1.png){width=726 height=434}\n:::\n:::\n\n\n## Differenzenformeln\n\n\n\nIn diesem Abschnitt werden Berechnungsformeln für die Approximation von \nAbleitungen durch Bildung von Funktionswertdifferenzen vorgestellt. Diese \nberuhen alle auf der Taylor-Entwicklung und können für beliebige Ableitungen \nund Ordnungen formuliert werden. Die einfachsten davon werden hier vorgestellt. \n\n### Erste Ableitung erster Ordnung\n\nDie einfachste Differenzenformel ergibt sich aus der Taylor-Reihe bis $\\mathsf{\\mathcal{O}(h^2)}$. Hier kann die Reihe direkt nach der gesuchten Ableitung an der Stelle $\\mathsf{x_0}$ umgeformt werden. \n\n$$\\mathsf{f(x_0 + h) = f(x_0) + f'(x_0)h + \\mathcal{O}(h^2)} $$\n$$\\mathsf{\\Rightarrow \\quad f'(x_0) = \\frac{f(x_0 + h) - f(x_0)}{h} + \\mathcal{O}(h)} $$\n\nDies ist die vorwärtsgerichtete Differenzformel erster Ordnung für die erste \nAbleitung. Erste Ordnung bedeutet hierbei, dass im Grenzwert \n$\\mathsf{h\\rightarrow 0}$ der Approximationsfehler linear mit der Schrittweite \nabnimmt.\n\nNach dieser Formel muss die abzuleitende Funktion an zwei Stellen $\\mathsf{f(x_0)}$ \nund $\\mathsf{f(x_0+h)}$ ausgewertet werden, um die Ableitung numerisch zu \nbestimmen. Im Grenzwert für eine beliebig kleine Schrittweite, d.h. \n$\\mathsf{h \\rightarrow 0}$, nähert sich dieser Quotient der exakten Ableitung \nan der Stelle $\\mathsf{x_0}$ an.\n\nDas folgende Beispiel demonstriert die Näherung anhand der Funktion\n\n$$ \\mathsf{f(x) = \\sin(3x) + 2x} $$\n\nDie Ableitung wird an der Stelle $\\mathsf{x_0 = 0.85}$ angenähert.\n\n::: {#44210353 .cell execution_count=6}\n``` {.python .cell-code}\ndef fkt(x):\n    return np.sin(3*x) + 2*x\n\n# Daten für die Visualisierung\nx = np.linspace(0, 2, 100)\ny = fkt(x)\n\n# Exakte Lösung bei x=0.85\nfp_exakt = 3*np.cos(3*0.85) + 2\n```\n:::\n\n\n::: {#f04b289e .cell execution_count=7}\n``` {.python .cell-code}\n# Entwicklungspunkt und Schrittweite\nh = 0.25\nx0 = 0.85\n\n# Auswertung an den beiden Stellen\nf0 = fkt(x0)\nfh = fkt(x0 + h)\n\n# Bestimmung der Ableitungsnäherung\nfp = (fh - f0) / h\n```\n:::\n\n\n::: {#936bd4e6 .cell execution_count=8}\n``` {.python .cell-code}\nprint(f\"Die numerische Näherung der Ableitung an der Stelle {x0:.2f}:\")\nprint(f\"Näherung mit Schrittweite {h:.2f}: {fp:.2f}\")\nprint(f\"Exakter Wert: {fp_exakt:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDie numerische Näherung der Ableitung an der Stelle 0.85:\nNäherung mit Schrittweite 0.25: -0.86\nExakter Wert: -0.49\n```\n:::\n:::\n\n\nDie Methode kann auch graphisch dargestellt werden. Die gesuchte Steigung ist \ndie Steigung der eingezeichneten Geraden.\n\n::: {#cde38dde .cell tags='[\"hide_input\"]' execution_count=9}\n``` {.python .cell-code}\nplt.plot(x, y, label=\"Funktion\")\nplt.scatter([x0], [f0], color='C3', label='Entwicklungspunkt', zorder=3)\nplt.scatter([x0+h], [fh], color='C4', label='Auswertepunkte', zorder=3)\n\nplt.vlines(x0, ymin=0, ymax=f0, color='C3', alpha=0.5)\n\nplt.plot(x, f0 + fp*(x-x0), label='Differenzenformel')\n\nplt.xlabel('x')\nplt.ylabel('y')\nplt.grid()\nplt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left');\n```\n\n::: {.cell-output .cell-output-display}\n![](intro_ableitungen_files/figure-html/cell-10-output-1.png){width=795 height=429}\n:::\n:::\n\n\n### Erste Ableitung zweiter Ordnung\n\nMit dem gleichen Ansatz kann auch eine Differenzenformel zweiter Ordnung \ngefunden werden. Dazu wird die Funktion an den Stellen $\\mathsf{x_0-h}$ und \n$\\mathsf{x_0+h}$ mit der Taylor-Reihe bis zur Ordnung $\\mathsf{\\mathcal{O}(h^3)}$ \napproximiert.\n\n$$\\mathsf{f(x_0+h) = f(x_0) + f'(x_0)\\cdot h + \\frac{1}{2}f''(x_0)\\cdot h^2 + \\mathcal{O}(h^3)} $$\n$$\\mathsf{f(x_0-h) = f(x_0) - f'(x_0)\\cdot h + \\frac{1}{2}f''(x_0)\\cdot h^2 + \\mathcal{O}(h^3)} $$\n\nDie Differenz dieser beiden Gleichungen führt zu\n\n$$\\mathsf{f(x_0+h) - f(x_0-h) = 2f'(x_0)\\cdot h + \\mathcal{O}(h^3)} $$\n\nUnd die Umformung nach der gesuchten Ableitung an der Stelle $\\mathsf{x_0}$ ergibt\n\n$$\\mathsf{f'(x_0) = \\frac{f(x_0+h) - f(x_0-h)}{2h} + \\mathcal{O}(h^2)} $$\n\nDies ist die zentrale Differenzenformel für die erste Ableitung zweiter Ordnung. \nWie bei der vorwärtsgerichteten Formel muss hier die Funktion an zwei Stellen \nausgewertet werden, jedoch nicht mehr am Entwicklungspunkt selbst. Durch diese \nSymmetrie bzgl. des Entwicklungspunkts ergibt sich ein besseres, hier \nquadratisches, Konvergenzverhalten.\n\n::: {#1b64b777 .cell execution_count=10}\n``` {.python .cell-code}\n# Auswertung an den beiden Stellen\nfnh = fkt(x0 - h)\nfph = fkt(x0 + h)\n\n# Bestimmung der Ableitungsnäherung\nfp = (fph - fnh) / (2*h)\n```\n:::\n\n\n::: {#f272a43e .cell execution_count=11}\n``` {.python .cell-code}\nprint(f\"Die numerische Näherung der Ableitung an der Stelle {x0:.2f}:\")\nprint(f\"Näherung mit Schrittweite {h:.2f}: {fp:.2f}\")\nprint(f\"Exakter Wert: {fp_exakt:.2f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDie numerische Näherung der Ableitung an der Stelle 0.85:\nNäherung mit Schrittweite 0.25: -0.26\nExakter Wert: -0.49\n```\n:::\n:::\n\n\nDie Methode kann auch graphisch dargestellt werden. Die gesuchte Steigung ist \ndie Steigung der eingezeichneten Geraden.\n\n::: {#8b8d85b4 .cell tags='[\"hide_input\"]' execution_count=12}\n``` {.python .cell-code}\nplt.plot(x, y, label=\"Funktion\")\nplt.scatter([x0], [f0], color='C3', label='Entwicklungspunkt', zorder=3)\nplt.scatter([x0-h, x0+h], [fnh, fph], color='C4', label='Auswertepunkte', zorder=3)\n\nplt.vlines(x0, ymin=0, ymax=f0, color='C3', alpha=0.5)\n\nplt.plot(x, fnh + fp*(x-x0+h), label='Differenzenformel')\n\nplt.xlabel('x')\nplt.ylabel('y')\nplt.grid()\nplt.legend(bbox_to_anchor=(1.05, 1.0), loc='upper left');\n```\n\n::: {.cell-output .cell-output-display}\n![](intro_ableitungen_files/figure-html/cell-13-output-1.png){width=795 height=429}\n:::\n:::\n\n\n## Zweite Ableitung zweiter Ordnung\n\nMit dem gleichen Schema wie oben, kann auch die Differenzenformel für die zweite \nAbleitung bestimmt werden. Diese lautet\n\n$$\\mathsf{f''(x_0) = \\frac{f(x_0-h) - 2f(x_0) + f(x_0+h)}{h^2} + \\mathcal{O}(h^2)}$$\n\n\n\n## Fehlerbetrachtung\n\n\n\nIn diesem Abschnitt werden die Approximationsfehler, d.h. Fehler aus der \nDifferenzenformeln, und Rundungsfehler, d.h. Fehler durch die endliche \nGenauigkeit der digitalen Darstellung von Zahlen, betrachtet.\n\n### Approximationsfehler\n\nDie Ordnung des Verfahrens kann durch die Betrachtung des Fehlers, hier zum \nbekannten exakten Wert, bestimmt werden. Dazu wird die Schrittweite \nkontinuierlich verkleinert. \n\n::: {#bc6e73ec .cell tags='[\"hide-input\"]' execution_count=14}\n``` {.python .cell-code code-fold=\"true\"}\ndef fkt(x):\n    return np.sin(3*x) + 2*x\n\n# Daten für die Visualisierung\nx = np.linspace(0, 2, 100)\ny = fkt(x)\n\n# Exakte Lösung bei x=1\nfp_exakt = 3*np.cos(3*0.85) + 2\n```\n:::\n\n\n::: {#0abc96e2 .cell tags='[\"hide-input\"]' execution_count=15}\n``` {.python .cell-code code-fold=\"true\"}\nx0 = 0.85\n\nhs = []\nfpfs = []\nfpcs = []\n\n\nh0 = 1\nfor i in range(18):\n    h = h0 / 2**i\n\n    f0 = fkt(x0)\n    fnh = fkt(x0 - h)\n    fph = fkt(x0 + h)\n\n    fpf = (fph - f0) / h\n    fpc = (fph - fnh) / (2*h)\n    \n    hs.append(h)\n    fpfs.append(fpf)\n    fpcs.append(fpc)\n```\n:::\n\n\n::: {#ab9e3beb .cell tags='[\"hide-input\"]' execution_count=16}\n``` {.python .cell-code code-fold=\"true\"}\nplt.plot(hs, np.abs(fpfs - fp_exakt), label='vorwärts')\nplt.plot(hs, np.abs(fpcs - fp_exakt), label='zentral')\n\nplt.plot([1e-5, 1e-1], [1e-5, 1e-1], '--', color='grey', label='Hilfslinien')\nplt.plot([1e-5, 1e-1], [1e-10, 1e-2], '--', color='grey')\n\nplt.xlabel('Schrittweite h')\nplt.ylabel('Fehler')\n\nplt.xscale('log')\nplt.yscale('log')\n\nplt.legend()\nplt.grid();\n```\n\n::: {.cell-output .cell-output-display}\n![](intro_ableitungen_files/figure-html/cell-17-output-1.png){width=599 height=431}\n:::\n:::\n\n\nIn der logiarithmischen Darstellung beider Achsen werden Potenzfunktionen zu \nGraden mit dem Potenzgrad als Steigung. Das bedeutet, dass der Fehler im obigen \nPlot sich wie eine Potenzfunktion mit dem Grad eins bzw. zwei verhält. Die \neingezeichneten Hilfslinien haben eine Steigung von eins bzw. zwei. Dies \nentspricht auch der Ordnung $\\mathsf{\\mathcal{O}(h)}$ bzw. $\\mathsf{\\mathcal{O}(h^2)}$ \naus der Differenzenformel.\n\n### Rundungsfehler\n\nWird nun die Schrittweiter noch weiter verkleinert, wirkt sich die Genauigkeit \nder Darstellung von Zahlen bzw. Rundungsfehler auf die Approximation aus.\n\n::: {#74443e0d .cell tags='[\"hide-input\"]' execution_count=17}\n``` {.python .cell-code code-fold=\"true\"}\nx0 = 0.85\n\nhs = []\nfpfs = []\nfpcs = []\n\n\nh0 = 1\nfor i in range(35):\n    h = h0 / 2**i\n\n    f0 = fkt(x0)\n    fnh = fkt(x0 - h)\n    fph = fkt(x0 + h)\n\n    fpf = (fph - f0) / h\n    fpc = (fph - fnh) / (2*h)\n    \n    hs.append(h)\n    fpfs.append(fpf)\n    fpcs.append(fpc)\n```\n:::\n\n\n::: {#ac81cbfc .cell tags='[\"hide-input\"]' execution_count=18}\n``` {.python .cell-code code-fold=\"true\"}\nplt.plot(hs, np.abs(fpfs - fp_exakt), label='vorwärts')\nplt.plot(hs, np.abs(fpcs - fp_exakt), label='zentral')\n\nplt.plot([1e-5, 1e-1], [1e-5, 1e-1], '--', color='grey', label='Hilfslinien')\nplt.plot([1e-5, 1e-1], [1e-10, 1e-2], '--', color='grey')\n\nplt.xlabel('Schrittweite h')\nplt.ylabel('Fehler')\n\nplt.xscale('log')\nplt.yscale('log')\n\nplt.legend()\nplt.grid();\n```\n\n::: {.cell-output .cell-output-display}\n![](intro_ableitungen_files/figure-html/cell-19-output-1.png){width=599 height=431}\n:::\n:::\n\n\nWie bereits vorgestellt, können 64-Bit-Zahlen nur mit einer Genauigkeit von \netwa $\\mathsf{\\epsilon\\approx10^{-16}}$ dargestellt werden. Das bedeutet, \ndass z.B. die Differenz von zwei Zahlen nicht genauer als $\\mathsf{\\epsilon}$ \nberechnet werden kann. Dies ist der sogenannte Rundungsfehler.\n\nIm konkreten Fall der Vorwärtsdifferenzenformel bedeutet dies:\n\n$$ \\mathsf{f'(x_0) = \\frac{f(x_0 + h) - f(x_0)}{h} + \\mathcal{O}(h)} $$\n$$\\mathsf{\\overset{Rundungsfehler}{\\Rightarrow} \\frac{f(x_0 + h) - f(x_0) + \\mathcal{O}(\\epsilon)}{h} + \\mathcal{O}(h)} $$\n$$ \\mathsf{= \\frac{f(x_0 + h) - f(x_0)}{h} + \\mathcal{O}\\left(\\frac{\\epsilon}{h}\\right) + \\mathcal{O}(h)} $$\n\nDamit macht eine Verkleinerung von $\\mathsf{h}$ nur Sinn, solange der Rundungsfehler \nklein gegenüber $\\mathsf{h}$ ist. Genauer:\n\n$$\\mathsf{\\frac{\\epsilon}{h} \\le h }$$\n$$\\mathsf{\\Rightarrow \\quad h \\ge \\sqrt{\\epsilon}} $$\n\nMit $\\mathsf{\\epsilon \\approx 10^{-16}}$ ist für diese Differenzenformel ein \n$\\mathsf{h}$ nur bis etwa $\\mathsf{10^{-8}}$ angemessen.\n\n",
    "supporting": [
      "intro_ableitungen_files"
    ],
    "filters": [],
    "includes": {}
  }
}